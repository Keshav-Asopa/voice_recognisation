{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read\n",
    "from sklearn.mixture import GMM \n",
    "from featureextraction import extract_features\n",
    "import speech_recognition as sr\n",
    "import pickle\n",
    "import shutil\n",
    "import os \n",
    "import numpy as np\n",
    "import time\n",
    "import pyaudio\n",
    "import wave\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_files():\n",
    "    for i in range(1,11):\n",
    "        global source1\n",
    "\n",
    "        FORMAT = pyaudio.paInt16\n",
    "        CHANNELS = 2\n",
    "        RATE = 44100\n",
    "        CHUNK = 1024\n",
    "        RECORD_SECONDS = 2\n",
    "        WAVE_OUTPUT_FILENAME = \"file\"+str(i)+\".wav\"\n",
    "\n",
    "        audio = pyaudio.PyAudio()\n",
    "\n",
    "        # start Recording\n",
    "        stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                    rate=RATE, input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "        print(\"say again\")\n",
    "        print(\"recording...\")\n",
    "        frames = []\n",
    "\n",
    "        for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)\n",
    "        print(\"finished recording\")\n",
    "\n",
    "        # stop Recording\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        audio.terminate()\n",
    "\n",
    "        #Saving the file\n",
    "        waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "        waveFile.setnchannels(CHANNELS)\n",
    "        waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        waveFile.setframerate(RATE)\n",
    "        waveFile.writeframes(b''.join(frames))\n",
    "        waveFile.close()\n",
    "        \n",
    "        #Moving file from one location to other location\n",
    "        #shutil.move('/home/keshav/Desktop/voice_recognisation/file'+str(i)+'.wav', '/home/keshav/Desktop/voice_recognisation/SampleData1/file'+str(i)+'.wav')\n",
    "\n",
    "        #time for prediction\n",
    "        #predict_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say again\n",
      "recording...\n",
      "finished recording\n",
      "say again\n",
      "recording...\n",
      "finished recording\n",
      "say again\n",
      "recording...\n",
      "finished recording\n",
      "say again\n",
      "recording...\n",
      "finished recording\n",
      "say again\n",
      "recording...\n",
      "finished recording\n",
      "say again\n",
      "recording...\n",
      "finished recording\n",
      "say again\n",
      "recording...\n",
      "finished recording\n",
      "say again\n",
      "recording...\n",
      "finished recording\n",
      "say again\n",
      "recording...\n",
      "finished recording\n",
      "say again\n",
      "recording...\n",
      "finished recording\n"
     ]
    }
   ],
   "source": [
    "making_files()\n",
    "for i in range(1,11):\n",
    "    #Moving file from one location to other location\n",
    "    shutil.move('/home/keshav/Desktop/voice_recognisation/file'+str(i)+'.wav', '/home/keshav/Desktop/voice_recognisation/trainingData1/user-1/file'+str(i)+'.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    #path to training data\n",
    "    source   = \"trainingData1/\"   \n",
    "\n",
    "    #path where training speakers will be saved\n",
    "    dest = \"Speakers_models1/\"\n",
    "    train_file = \"trainingDataPath1.txt\"        \n",
    "    file_paths = open(train_file,'r')\n",
    "    print(type(file_paths))\n",
    "\n",
    "    count = 1\n",
    "    # Extracti(ng features for each speaker (5 files per speakers)\n",
    "    features = np.asarray(())\n",
    "\n",
    "    print(file_paths)\n",
    "    for path in file_paths:\n",
    "        path = path.strip()   \n",
    "        print(path)\n",
    "\n",
    "        # read the audio\n",
    "        sr,audio = read(source + path)\n",
    "\n",
    "        # extract 40 dimensional MFCC & delta MFCC features\n",
    "        vector   = extract_features(audio,sr)\n",
    "\n",
    "        if features.size == 0:\n",
    "            features = vector\n",
    "        else:\n",
    "            features = np.vstack((features, vector))\n",
    "\n",
    "        if count == 10:    \n",
    "\n",
    "            gmm = GMM(n_components = 16, n_iter = 200, covariance_type='diag',n_init = 3)        \n",
    "\n",
    "            gmm.fit(features)\n",
    "\n",
    "            # dumping the trained gaussian model\n",
    "            picklefile = path.split(\"-\")[0]+\".gmm\"\n",
    "            print(picklefile)\n",
    "            \n",
    "            pickle.dump(gmm,open(dest + picklefile,'wb'))\n",
    "            print('+ modeling completed for speaker:',picklefile,\" with data point = \",features.shape)    \n",
    "            features = np.asarray(())\n",
    "            break\n",
    "            \n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.TextIOWrapper'>\n",
      "<_io.TextIOWrapper name='trainingDataPath.txt' mode='r' encoding='UTF-8'>\n",
      "keshav-1/keshav1.wav\n",
      "keshav-1/keshav2.wav\n",
      "keshav-1/keshav3.wav\n",
      "keshav-1/keshav4.wav\n",
      "keshav-1/keshav5.wav\n",
      "keshav-1/keshav6.wav\n",
      "keshav-1/keshav7.wav\n",
      "keshav-1/keshav8.wav\n",
      "keshav-1/keshav9.wav\n",
      "keshav-1/keshav10.wav\n",
      "keshav.gmm\n",
      "+ modeling completed for speaker: keshav.gmm  with data point =  (3980, 40)\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file1.wav', 'file2.wav', 'file3.wav', 'file4.wav', 'file5.wav', 'file6.wav', 'file7.wav', 'file8.wav', 'file9.wav', 'file10.wav']\n"
     ]
    }
   ],
   "source": [
    "WAVE_OUTPUT_FILENAME = []\n",
    "for i in range(1,11):\n",
    "    a = \"file\"+str(i)+\".wav\"\n",
    "    WAVE_OUTPUT_FILENAME.append(a)\n",
    "print(WAVE_OUTPUT_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
